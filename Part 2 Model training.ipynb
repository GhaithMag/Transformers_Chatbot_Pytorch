{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb60d17",
   "metadata": {},
   "source": [
    "# Classes for Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de41ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.pairs = json.load(open('pairs_encoded.json'))\n",
    "        self.dataset_size = len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        question = torch.tensor(self.pairs[i][0], dtype=torch.long)\n",
    "        reply = torch.tensor(self.pairs[i][1], dtype=torch.long)\n",
    "        return question, reply\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c526f",
   "metadata": {},
   "source": [
    "\n",
    "<p>The <code>Dataset</code>  class is responsible for loading our preprocessed and encoded question-reply pairs from a JSON file. It also provides methods to access individual data points and to get the length of the dataset.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecaf6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    (questions, replies) = zip(*batch)\n",
    "    question_lens = [len(x) for x in questions]\n",
    "    reply_lens = [len(x) for x in replies]\n",
    "    \n",
    "    questions = [torch.Tensor(x) for x in questions]\n",
    "    replies = [torch.Tensor(x) for x in replies]\n",
    "    \n",
    "    questions_padded = pad_sequence(questions, batch_first=True, padding_value=0)\n",
    "    replies_padded = pad_sequence(replies, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return questions_padded, replies_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b35aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(Dataset(),\n",
    "                                           batch_size = 100, \n",
    "                                           shuffle=True, \n",
    "                                           pin_memory=True,collate_fn=pad_collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6635c",
   "metadata": {},
   "source": [
    "<p>The <code>train_loader</code> is an instance of PyTorch's DataLoader class, which makes it easier to feed the training data into the model during training. Here's what each argument does:</p>\n",
    "<ul>\n",
    "    <li><code>Dataset()</code>: This is the custom dataset class we defined earlier. It loads the question-answer pairs and prepares them for training.</li>\n",
    "    <li><code>batch_size = 100</code>: This specifies that we want to use 100 question-answer pairs in each batch of training.</li>\n",
    "    <li><code>shuffle=True</code>: This shuffles the data before each epoch, which can often help the model learn better.</li>\n",
    "    <li><code>pin_memory=True</code>: This argument is used for faster data transfer between CPU and GPU.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c9227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(question, reply_input, reply_target):\n",
    "    def subsequent_mask(size):\n",
    "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.bool)\n",
    "        return mask.unsqueeze(0)\n",
    "\n",
    "    question_mask = (question != 0).to(device).unsqueeze(1).unsqueeze(1)\n",
    "    reply_input_mask = (reply_input != 0).unsqueeze(1) & subsequent_mask(reply_input.size(-1)).type_as(reply_input.data)\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)\n",
    "    reply_target_mask = reply_target != 0\n",
    "    return question_mask, reply_input_mask, reply_target_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289b6f4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p>The <code>create_masks</code> function generates masks for the question and reply sequences. These masks are used later in the transformer model to ignore certain words during the self-attention mechanism</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7924d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements embeddings of the words and adds their positional encodings. \n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len = 50, num_layers = 6):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = self.create_positinal_encoding(max_len, self.d_model)     # (1, max_len, d_model)\n",
    "        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def create_positinal_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        for pos in range(max_len):   # for each position of the word\n",
    "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)   # include the batch size\n",
    "        return pe\n",
    "        \n",
    "    def forward(self, embedding, layer_idx):\n",
    "        if layer_idx == 0:\n",
    "            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n",
    "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
    "        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n",
    "        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n",
    "        embedding = self.dropout(embedding)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c408e7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p>The <code>Embeddings</code> class is a PyTorch module responsible for handling the word embeddings and adding positional encodings. This is crucial for the Transformer model to understand the sequence and semantics of the input data.</p>\n",
    "\n",
    "<h4>Class Initialization (__init__ method)</h4>\n",
    "<p>The class constructor initializes the following:</p>\n",
    "<ul>\n",
    "    <li><code>d_model</code>: The dimension of the word embeddings.</li>\n",
    "    <li><code>embed</code>: The actual embedding layer.</li>\n",
    "    <li><code>pe</code>: Positional encoding for the sequence length, precomputed for efficiency.</li>\n",
    "    <li><code>te</code>: Positional encoding for the number of layers, also precomputed.</li>\n",
    "    <li><code>dropout</code>: A dropout layer for regularization.</li>\n",
    "</ul>\n",
    "\n",
    "<h4>create_positional_encoding Method</h4>\n",
    "<p>This method generates the positional encodings based on the formula provided in the original Transformer paper. The positional encoding is added to give the model information about the position of each word in the sequence.</p>\n",
    "\n",
    "<h4>forward Method</h4>\n",
    "<p>This is where the actual computation happens:</p>\n",
    "<ul>\n",
    "    <li>If it's the first layer (<code>layer_idx == 0</code>), the word embeddings are computed and scaled by the square root of their dimension.</li>\n",
    "    <li>The positional encoding for the sequence length is added to the embeddings.</li>\n",
    "    <li>The positional encoding for the current layer is also added. This is unique to this implementation and not a standard part of the Transformer model.</li>\n",
    "    <li>Dropout is applied for regularization.</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Summary</h4>\n",
    "<p>Overall, the <code>Embeddings</code> class is a key component for handling the input data in the Transformer model. It ensures that both the semantics (via embeddings) and the sequence information (via positional encodings) are effectively captured.</p>\n",
    "\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae04a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, d_model):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query, key, value of shape: (batch_size, max_len, 512)\n",
    "        mask of shape: (batch_size, 1, 1, max_words)\n",
    "        \"\"\"\n",
    "        # (batch_size, max_len, 512)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)   \n",
    "        \n",
    "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        \n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
    "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
    "        weights = self.dropout(weights)\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
    "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "        # (batch_size, max_len, h * d_k)\n",
    "        interacted = self.concat(context)\n",
    "        return interacted "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce352d39",
   "metadata": {},
   "source": [
    "\n",
    "<p>The <code>MultiHeadAttention</code> class is an implementation of the multi-head attention mechanism, a crucial component in the Transformer model. This class is responsible for computing the attention weights and applying them to the input sequences.</p>\n",
    "\n",
    "<h4>Class Initialization (__init__ method)</h4>\n",
    "<p>The constructor initializes the following:</p>\n",
    "<ul>\n",
    "    <li><code>d_model</code>: The dimension of the input embeddings.</li>\n",
    "    <li><code>heads</code>: The number of attention heads.</li>\n",
    "    <li><code>d_k</code>: The dimension of the keys, queries, and values.</li>\n",
    "    <li>Linear layers for transforming the input queries, keys, and values.</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Forward Method</h4>\n",
    "<p>This method performs the following operations:</p>\n",
    "<ul>\n",
    "    <li>Transforms the queries, keys, and values using the initialized linear layers.</li>\n",
    "    <li>Splits these into multiple heads.</li>\n",
    "    <li>Computes the attention scores and applies masking.</li>\n",
    "    <li>Computes the weighted sum of values based on the attention scores.</li>\n",
    "    <li>Concatenates the multiple heads back into a single array.</li>\n",
    "</ul>\n",
    "\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, middle_dim = 2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce87850f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p>The <code>FeedForward</code> class is an implementation of the position-wise feed-forward networks, another key component in the Transformer model. This class is responsible for applying a two-layer fully connected network to each position in the input sequence.</p>\n",
    "\n",
    "<h4>Class Initialization (__init__ method)</h4>\n",
    "<p>The constructor initializes the following:</p>\n",
    "<ul>\n",
    "    <li><code>d_model</code>: The dimension of the input embeddings.</li>\n",
    "    <li><code>middle_dim</code>: The dimension of the middle layer.</li>\n",
    "    <li>Two linear layers for transforming the input.</li>\n",
    "    <li>Dropout layer for regularization.</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Forward Method</h4>\n",
    "<p>This method performs the following operations:</p>\n",
    "<ul>\n",
    "    <li>Applies the first linear layer followed by a ReLU activation.</li>\n",
    "    <li>Applies dropout for regularization.</li>\n",
    "    <li>Applies the second linear layer to produce the output.</li>\n",
    "</ul>\n",
    "\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb25f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2385fc12",
   "metadata": {},
   "source": [
    "<p>The <code>EncoderLayer</code> class represents a single layer within the encoder part of the Transformer model. Each encoder layer consists of two main parts: a multi-head self-attention mechanism and a position-wise feed-forward neural network.</p>\n",
    "\n",
    "<h4>Class Initialization (__init__ method)</h4>\n",
    "<p>The constructor initializes the following components:</p>\n",
    "<ul>\n",
    "    <li><code>d_model</code>: The dimension of the input embeddings.</li>\n",
    "    <li><code>heads</code>: The number of attention heads.</li>\n",
    "    <li><code>Layer Normalization</code>: To normalize the outputs of the self-attention and feed-forward neural network.</li>\n",
    "    <li><code>Multi-Head Attention</code>: For the self-attention mechanism.</li>\n",
    "    <li><code>Feed-Forward Neural Network</code>: Implemented as a separate class.</li>\n",
    "    <li><code>Dropout: For regularization</code>.</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Forward Method</h4>\n",
    "<p>This method performs the following operations:</p>\n",
    "<ul>\n",
    "    <li>Applies multi-head self-attention and adds the input (residual connection), followed by layer normalization.</li>\n",
    "    <li>Applies the feed-forward neural network and adds the input (another residual connection), followed by another layer normalization.</li>\n",
    "</ul>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee537fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
    "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
    "        query = self.layernorm(query + embeddings)\n",
    "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
    "        interacted = self.layernorm(interacted + query)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        decoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cded06f",
   "metadata": {},
   "source": [
    "\n",
    "<p>The <code>DecoderLayer</code> class represents a single layer within the decoder part of the Transformer model. Each decoder layer consists of three main parts: a masked multi-head self-attention mechanism, an encoder-decoder attention mechanism, and a position-wise feed-forward neural network.</p>\n",
    "\n",
    "<h4>Class Initialization (__init__ method)</h4>\n",
    "<p>The constructor initializes the following components:</p>\n",
    "<ul>\n",
    "    <li><code>d_model</code>: The dimension of the input embeddings.</li>\n",
    "    <li><code>heads</code>: The number of attention heads.</li>\n",
    "    <li>Layer Normalization: To normalize the outputs of the attention mechanisms and feed-forward neural network.</li>\n",
    "    <li>Self Multi-Head Attention: For the masked self-attention mechanism.</li>\n",
    "    <li>Source Multi-Head Attention: For the encoder-decoder attention mechanism.</li>\n",
    "    <li>Feed-Forward Neural Network: Implemented as a separate class.</li>\n",
    "    <li>Dropout: For regularization.</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Forward Method</h4>\n",
    "<p>This method performs the following operations:</p>\n",
    "<ul>\n",
    "    <li>Applies masked multi-head self-attention and adds the input (residual connection), followed by layer normalization.</li>\n",
    "    <li>Applies encoder-decoder attention and adds the input (another residual connection), followed by another layer normalization.</li>\n",
    "    <li>Applies the feed-forward neural network and adds the input (yet another residual connection), followed by a final layer normalization.</li>\n",
    "</ul>\n",
    "\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads, num_layers, word_map):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = len(word_map)\n",
    "        self.embed = Embeddings(self.vocab_size, d_model, num_layers=num_layers)\n",
    "        \n",
    "        # Create a list of unique EncoderLayer and DecoderLayer instances\n",
    "        self.encoders = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.decoders = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        \n",
    "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
    "        \n",
    "    def encode(self, src_embeddings, src_mask):\n",
    "        for i in range(self.num_layers):\n",
    "            src_embeddings = self.embed(src_embeddings, i)\n",
    "            src_embeddings = self.encoders[i](src_embeddings, src_mask)  # Use the i-th encoder layer\n",
    "        return src_embeddings\n",
    "    \n",
    "    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n",
    "        for i in range(self.num_layers):\n",
    "            tgt_embeddings = self.embed(tgt_embeddings, i)\n",
    "            tgt_embeddings = self.decoders[i](tgt_embeddings, src_embeddings, src_mask, target_mask)  # Use the i-th decoder layer\n",
    "        return tgt_embeddings\n",
    "        \n",
    "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
    "        encoded = self.encode(src_words, src_mask)\n",
    "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
    "        out = F.log_softmax(self.logit(decoded), dim=2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5f2d5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p>The <code>Transformer</code> class encapsulates the entire Transformer model, including the encoder and decoder. It serves as the main interface for both encoding the source sequence and decoding the target sequence.</p>\n",
    "\n",
    "<h4>Class Initialization (__init__ method)</h4>\n",
    "<p>The constructor initializes the following components:</p>\n",
    "<ul>\n",
    "    <li><code>d_model</code>: The dimension of the input embeddings.</li>\n",
    "    <li><code>heads</code>: The number of attention heads.</li>\n",
    "    <li><code>num_layers</code>: The number of layers in both the encoder and decoder.</li>\n",
    "    <li><code>word_map</code>: A mapping from words to their corresponding indices.</li>\n",
    "    <li>Embeddings: Word embeddings and positional encodings.</li>\n",
    "    <li>Encoder and Decoder: Implemented as separate classes.</li>\n",
    "    <li>Output Linear Layer: To produce the final output probabilities.</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Encode Method</h4>\n",
    "<p>This method performs the encoding of the source sequence. It applies the embeddings and then passes the source sequence through multiple encoder layers.</p>\n",
    "\n",
    "<h4>Decode Method</h4>\n",
    "<p>This method performs the decoding of the target sequence. It applies the embeddings and then passes the target sequence through multiple decoder layers.</p>\n",
    "\n",
    "<h4>Forward Method</h4>\n",
    "<p>This method is the main interface for the model and performs both encoding and decoding. It takes the source sequence, source mask, target sequence, and target mask as inputs, and returns the output probabilities for the target sequence.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWarmup:\n",
    "    \n",
    "    def __init__(self, model_size, warmup_steps, optimizer):\n",
    "        \n",
    "        self.model_size = model_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.lr = 0\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
    "        \n",
    "    def step(self):\n",
    "        # Increment the number of steps each time we call the step function\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        # update the learning rate\n",
    "        self.lr = lr\n",
    "        self.optimizer.step()       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83978b14",
   "metadata": {},
   "source": [
    "\n",
    "<p>The <code>AdamWarmup</code> class is responsible for implementing the learning rate scheduling that's commonly used in training Transformer models. This scheduler dynamically adjusts the learning rate during training based on the number of steps taken and a warm-up period. The learning rate increases for the first <code>warmup_steps</code> training steps, and decreases thereafter. This is particularly useful for stabilizing the training of Transformers.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cca7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossWithLS(nn.Module):\n",
    "\n",
    "    def __init__(self, size, smooth):\n",
    "        super(LossWithLS, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
    "        self.confidence = 1.0 - smooth\n",
    "        self.smooth = smooth\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, prediction, target, mask):\n",
    "        \"\"\"\n",
    "        prediction of shape: (batch_size, max_words, vocab_size)\n",
    "        target and mask of shape: (batch_size, max_words)\n",
    "        \"\"\"\n",
    "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
    "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
    "        mask = mask.float()\n",
    "        mask = mask.view(-1)       # (batch_size * max_words)\n",
    "        labels = prediction.data.clone()\n",
    "        labels.fill_(self.smooth / (self.size - 1))\n",
    "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
    "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e4912",
   "metadata": {},
   "source": [
    "\n",
    "<p>The <code>LossWithLS</code> class implements label-smoothed Kullback-Leibler divergence loss. Label smoothing is a regularization technique that prevents the model from becoming too confident about the labels during training. This is particularly useful for improving the model's generalization performance. The class takes the predicted probabilities, the target labels, and a mask to compute the loss only over the unmasked elements.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b981778",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41585973",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "heads = 8\n",
    "num_layers = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 10\n",
    "\n",
    "with open('WORDMAP_corpus.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "    \n",
    "transformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map)\n",
    "transformer = transformer.to(device)\n",
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
    "criterion = LossWithLS(len(word_map), 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd9af5",
   "metadata": {},
   "source": [
    "<h4>Model Initialization and Preparation for Training</h4>\n",
    "<p>This section of the code initializes the Transformer model with specific hyperparameters such as <code>d_model</code>, <code>heads</code>, and <code>num_layers</code>. The model is then moved to the appropriate computing device, either a GPU or CPU, depending on availability.</p>\n",
    "\n",
    "<h4>Optimizer Setup</h4>\n",
    "<p>The Adam optimizer is used for training the model. A warm-up schedule is also applied to the learning rate using the <code>AdamWarmup</code> class. This helps in stabilizing the training process.</p>\n",
    "\n",
    "<h4>Loss Function</h4>\n",
    "<p>The loss function used is a Kullback-Leibler divergence loss with label smoothing, implemented by the <code>LossWithLS</code> class. Label smoothing helps in regularizing the model and improving its generalization performance.</p>\n",
    "\n",
    "<h4>Word Map Loading</h4>\n",
    "<p>The word map, which is a mapping between words and their corresponding integer indices, is loaded from a JSON file. This word map is used for encoding and decoding sequences.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(train_loader, transformer, criterion, epoch):\n",
    "    \n",
    "    transformer.train()\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    # Initialize tqdm for progress bar\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    \n",
    "    for i, (question, reply) in pbar:\n",
    "        \n",
    "        samples = question.shape[0]\n",
    "\n",
    "        # Move to device\n",
    "        question = question.to(device)\n",
    "        reply = reply.to(device)\n",
    "\n",
    "        # Prepare Target Data\n",
    "        reply_input = reply[:, :-1]\n",
    "        reply_target = reply[:, 1:]\n",
    "\n",
    "        # Create mask and add dimensions\n",
    "        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n",
    "\n",
    "        # Get the transformer outputs\n",
    "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(out, reply_target, reply_target_mask)\n",
    "        \n",
    "        # Backprop\n",
    "        transformer_optimizer.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        transformer_optimizer.step()\n",
    "        \n",
    "        sum_loss += loss.item() * samples\n",
    "        count += samples\n",
    "        \n",
    "        # Update tqdm\n",
    "        pbar.set_description(f\"Epoch [{epoch}][{i}/{len(train_loader)}]\\tLoss: {sum_loss/count:.4f}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch}] completed. Average Loss: {sum_loss/count:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac7c62",
   "metadata": {},
   "source": [
    "\n",
    "<p>The <code>train</code> function is responsible for training the Transformer model for one epoch. It iterates through the training data, computes the loss, and updates the model parameters. We use tqdm to display a progress bar during training.</p>\n",
    "<p>We calculate the loss at each step and display the average loss at the end of each epoch. Note that for chatbots, accuracy is not a standard metric. Instead, metrics like BLEU score or perplexity are often used, typically during the evaluation phase.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212177f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(transformer, question, question_mask, max_len, word_map):\n",
    "    \"\"\"\n",
    "    Performs Greedy Decoding with a batch size of 1\n",
    "    \"\"\"\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}\n",
    "    transformer.eval()\n",
    "    start_token = word_map['<start>']\n",
    "    encoded = transformer.encode(question, question_mask)\n",
    "    words = torch.LongTensor([[start_token]]).to(device)\n",
    "    \n",
    "    for step in range(max_len - 1):\n",
    "        size = words.shape[1]\n",
    "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
    "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
    "        predictions = transformer.logit(decoded[:, -1])\n",
    "        _, next_word = torch.max(predictions, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        if next_word == word_map['<end>']:\n",
    "            break\n",
    "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
    "        \n",
    "    # Construct Sentence\n",
    "    if words.dim() == 2:\n",
    "        words = words.squeeze(0)\n",
    "        words = words.tolist()\n",
    "        \n",
    "    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n",
    "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e952ff",
   "metadata": {},
   "source": [
    "<p>The <code>evaluate</code> function performs greedy decoding to generate a reply for a given question. The function takes the following parameters:</p>\n",
    "<ul>\n",
    "    <li><code>transformer</code>: The trained Transformer model.</li>\n",
    "    <li><code>question</code>: The input question tensor.</li>\n",
    "    <li><code>question_mask</code>: The mask for the input question.</li>\n",
    "    <li><code>max_len</code>: The maximum length for the generated reply.</li>\n",
    "    <li><code>word_map</code>: A dictionary mapping words to their corresponding indices.</li>\n",
    "</ul>\n",
    "<p>It returns a sentence generated by the model as a reply to the input question.</p>\n",
    "<p>The function first encodes the question using the Transformer's encoder. Then, it decodes the encoded question into a reply sentence using the Transformer's decoder. The decoding is done one word at a time, and the function uses greedy decoding to select the most likely next word at each step.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_interval = 100  # Save the model every 100 epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train(train_loader, transformer, criterion, epoch)\n",
    "    \n",
    "    if epoch % save_interval == 0:\n",
    "        state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
    "        torch.save(state, f'checkpoint_{epoch}.pth.tar')\n",
    "        last_save = str(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8b165",
   "metadata": {},
   "source": [
    "<p>This section of the code defines the main training loop for the Transformer model. It iterates through a specified number of epochs, calling the <code>train</code> function at each iteration to train the model on the training data.</p>\n",
    "<p>Additionally, the model and its optimizer's state are saved as a checkpoint file every 100 epochs. This is useful for long training runs, as it allows you to resume training from a saved state, rather than starting over. The checkpoint is saved in a file named <code>checkpoint_{epoch}.pth.tar</code>, where <code>{epoch}</code> is the current epoch number.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0839748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoint_'+last_save+'.pth.tar')\n",
    "transformer = checkpoint['transformer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec77e1a8",
   "metadata": {},
   "source": [
    "<p>This section of the code is responsible for loading a previously saved model checkpoint. This is particularly useful for resuming training or for inference. Here's a breakdown:</p>\n",
    "<ul>\n",
    "    <li><code>torch.load('checkpoint_'+last_save+'.pth.tar')</code>: This line loads the saved checkpoint file from the disk. The variable <code>last_save</code> presumably contains the epoch number or some identifier for the last saved state.</li>\n",
    "    <li><code>checkpoint = ...</code>: The loaded checkpoint is stored in the variable named <code>checkpoint</code>.</li>\n",
    "    <li><code>transformer = checkpoint['transformer']</code>: The Transformer model's state_dict is extracted from the checkpoint and loaded into the <code>transformer</code> model. This effectively updates the model with the saved weights and biases.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(1):\n",
    "    question = input(\"Question: \") \n",
    "    if question == 'quit':\n",
    "        break\n",
    "    max_len = input(\"Maximum Reply Length: \")\n",
    "    enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "    question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "    sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bae7cb",
   "metadata": {},
   "source": [
    "<p>This section of the code sets up an interactive loop that allows the user to chat with the trained Transformer model. Here's how it works:</p>\n",
    "<ul>\n",
    "    <li><code>while(1):</code>: An infinite loop to keep the chat session running until the user decides to quit.</li>\n",
    "    <li><code>question = input(\"Question: \")</code>: The code prompts the user to input a question.</li>\n",
    "    <li><code>if question == 'quit': break</code>: If the user types 'quit', the loop breaks, and the chat session ends.</li>\n",
    "    <li><code>max_len = input(\"Maximum Reply Length: \")</code>: The user is prompted to specify the maximum length for the model's reply.</li>\n",
    "    <li><code>enc_qus = ...</code>: The question is tokenized and converted into a tensor of integers based on the word map.</li>\n",
    "    <li><code>question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)</code>: The tokenized question is converted into a PyTorch tensor and moved to the specified device (CPU or GPU).</li>\n",
    "    <li><code>question_mask = ...</code>: A mask is created for the question tensor to indicate which elements are not padding.</li>\n",
    "    <li><code>sentence = evaluate(...)</code>: The <code>evaluate</code> function is called to generate a reply from the model.</li>\n",
    "    <li><code>print(sentence)</code>: The generated reply is printed to the console.</li>\n",
    "</ul>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
